# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oDQ8wF_Q2QCCE_XB2Zsr5HcCTg0H_zCw
"""

import time
import traceback
import pyotp
import xlwings as xw
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
from NorenRestApiPy.NorenApi import NorenApi
from collections import defaultdict
from datetime import datetime
import requests
import zipfile
import os
import queue
import threading
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import pandas as pd
pd.set_option('future.no_silent_downcasting', True)

# Function to download and extract files
def download_and_extract(url):
    """Download a zip file from the given URL and extract it to the specified path."""
    try:
        response = requests.get(url)
        if response.status_code == 200:
            zip_path = os.path.join(os.path.basename(url))
            with open(zip_path, "wb") as file:
                file.write(response.content)

            # Extract the zip file
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall()
            print("Extracted symbols")
        else:
            print(f"Failed to download {url}. Status code: {response.status_code}")
    except Exception as e:
        print(f"Error downloading/extracting {url}: {e}")

# Download and extract NFO and BFO symbols
download_and_extract("https://api.shoonya.com/NFO_symbols.txt.zip")
download_and_extract("https://api.shoonya.com/BFO_symbols.txt.zip")

class ShoonyaApiPy(NorenApi):
    def __init__(self):
        NorenApi.__init__(self, host='https://api.shoonya.com/NorenWClientTP/', websocket='wss://api.shoonya.com/NorenWSTP/')

api = ShoonyaApiPy()

# Create a message queue for tick data
tick_queue = queue.Queue()

# Create a queue for Excel updates
excel_queue = queue.Queue()

# Ensure Excel file exists (macro-enabled format)
excel_file = "option_futures.xlsm"
if not os.path.exists(excel_file):
    print(f"Excel file '{excel_file}' does not exist. Creating a new macro-enabled workbook...")
    wb = xw.Book()
    wb.save(excel_file)
    print(f"Created new macro-enabled workbook: {excel_file}")
else:
    print(f"Excel file '{excel_file}' already exists.")
    wb = xw.Book(excel_file)

# Define indices and their tokens
indices = {
       "FINNIFTY": {"token": "26037", "sheet": "finnifty"},
    "MIDCPNIFTY": {"token": "26074", "sheet": "midcpnifty"},
    "SENSEX": {"token": "BSXOPT", "sheet": "sensex"}
}

# Function to ensure sheets exist
def ensure_sheets_exist(wb, indices):
    # Add a new sheet named 'Control' if it doesn't exist
    if "Control" not in [sheet.name for sheet in wb.sheets]:
        print("Sheet 'Control' does not exist. Creating...")
        wb.sheets.add("Control")

    # Ensure sheets for indices exist
    for index, details in indices.items():
        sheet_name = details["sheet"]
        if sheet_name not in [sheet.name for sheet in wb.sheets]:
            print(f"Sheet '{sheet_name}' does not exist. Creating...")
            wb.sheets.add(sheet_name)
    return {index: wb.sheets[details["sheet"]] for index, details in indices.items()}

# Ensure sheets exist
print("Ensuring sheets exist...")
sheets = ensure_sheets_exist(wb, indices)
print("Sheets ensured.")

def read_control_parameters(wb):
    control_sheet = wb.sheets["Control"]
    sensex_spot = control_sheet.range("B8").value

    # Handle case where fetch failed is shown in Excel
    if isinstance(sensex_spot, str) and "Fetch Failed" in sensex_spot:
        sensex_spot = 0  # Will use the last known good value

    control_params = {
        "expiry_dates_count": int(control_sheet.range("B2").value or 1),
        "sleep_time": int(control_sheet.range("B3").value or 1),
        "get_quotes_count": int(control_sheet.range("B4").value or 10),
        "userid": str(control_sheet.range("B5").value or ""),
        "password": str(control_sheet.range("B6").value or ""),
        "totp_key": str(control_sheet.range("B7").value or ""),
        "sensex_spot_price": float(sensex_spot or 0),  # Handle None or string values
        "update_interval": 10,
        "sensex_update_interval": 240,
        "market_data_refresh_interval": 3  # Added new parameter for market data refresh
    }
    return control_params

# Initialize the spot update trackers
last_sensex_update = 0
current_sensex_price = 0
last_market_data_refresh = 0

# Read control parameters
print("Reading control parameters...")
control_params = read_control_parameters(wb)
print("Control parameters:", control_params)

# Login using credentials from the 'Control' sheet
def shoonya_login():
    global api, feed_opened
    try:
        # Generate TOTP
        totp = pyotp.TOTP(control_params["totp_key"]).now()

        # Login to Shoonya API
        ret = api.login(
            userid=control_params["userid"],
            password=control_params["password"],
            twoFA=totp,
            vendor_code="FN100758_U",
            api_secret="e2fd2b1eb4da1c54ed4a84e04dc592ea",
            imei="abc1234"
        )
        if ret.get("stat") != "Ok":
            print(f"Login failed: {ret.get('emsg', 'Unknown error')}")
            exit(1)
        print("Login successful.")
    except Exception as e:
        print(f"Error during login: {e}")
        exit(1)

# Perform login
shoonya_login()

# Read symbols from text file and group by index and expiry date
def read_symbols(file_path, index):
    symbols = []
    with open(file_path, 'r') as file:
        next(file)  # Skip header
        for line in file:
            parts = line.strip().split(',')
            if parts[3] == index:  # Filter by index
                expiry_date = datetime.strptime(parts[5], "%d-%b-%Y")
                symbols.append({
                    "exch": parts[0],
                    "token": parts[1],
                    "tsym": parts[4],
                    "exd": expiry_date,
                    "instname": parts[6],
                    "optt": parts[7],
                    "strprc": parts[8]
                })
    return symbols

# Function to safely write to Excel
def safe_excel_write(wb, sheet_name, data, start_cell='B3'):
    try:
        sheet = wb.sheets[sheet_name]
        sheet.range(start_cell).options(index=False, header=True).value = data
        return True
    except Exception as e:
        print(f"Error writing to {sheet_name}: {e}")
        return False

def update_sensex_spot_in_excel(wb, spot_price):
    try:
        control_sheet = wb.sheets["Control"]
        # If spot_price is a number, format it properly
        if isinstance(spot_price, (int, float)):
            control_sheet.range("B8").value = float(spot_price)
            print(f"Updated Sensex spot price in Excel: {spot_price}")
        else:
            # Handle string messages like "Fetch Failed"
            control_sheet.range("B8").value = spot_price
            print(f"Updated Sensex spot status in Excel: {spot_price}")
        return True
    except Exception as e:
        print(f"Error updating Sensex spot price in Excel: {e}")
        return False

def update_sensex_spot_price():
    global current_sensex_price, last_sensex_update

    # Check if it's time to update the Sensex spot price
    current_time = time.time()
    if current_time - last_sensex_update >= control_params["sensex_update_interval"]:
        scraped_price = None
        scrape_successful = False

        # First try to scrape the live price
        print("Attempting to scrape live Sensex spot price...")
        try:
            # Initialize Chrome options for headless browsing
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--window-size=1920,1080")

            driver = webdriver.Chrome(options=chrome_options)

            try:
                print("Opening BSE website...")
                driver.get('https://www.bseindia.com/')

                # Wait until the spot price is loaded
                print("Waiting for spot price element...")
                wait = WebDriverWait(driver, 10)
                spot_price_element = wait.until(EC.presence_of_element_located((By.ID, 'tdsp')))
                spot_price_text = spot_price_element.text
                print(f"Found spot price text: {spot_price_text}")

                # Clean the price text (remove commas) and convert to float
                scraped_price = float(spot_price_text.replace(',', ''))
                scrape_successful = True
                print(f"Successfully scraped Sensex spot price: {scraped_price}")

            except Exception as e:
                print(f"Failed to scrape Sensex spot price: {str(e)}")
                traceback.print_exc()

            finally:
                driver.quit()
                print("Web driver closed")

        except Exception as e:
            print(f"Error in Selenium setup: {str(e)}")
            traceback.print_exc()

        # Determine which price to use
        if scrape_successful and scraped_price is not None and scraped_price > 0:
            # Use the scraped price if available and valid
            current_sensex_price = scraped_price
            # Update Excel with the new price
            update_sensex_spot_in_excel(wb, scraped_price)
            print(f"Using live scraped Sensex price: {scraped_price}")
        else:
            # Fall back to Excel value if scraping failed
            excel_price = control_params["sensex_spot_price"]
            if excel_price > 0:
                current_sensex_price = excel_price
                print(f"Scraping failed! Using Sensex spot price from Excel: {excel_price}")
                # Update Excel to show fetch failed but keep the existing value
                update_sensex_spot_in_excel(wb, f"Fetch Failed (Using: {excel_price})")
            else:
                current_sensex_price = 0
                print("Scraping failed and no valid Excel price available!")
                update_sensex_spot_in_excel(wb, "Fetch Failed")

        last_sensex_update = current_time

    return current_sensex_price

# Add these global variables at the top
market_data_ready = False
market_data_lock = threading.Lock()

def refresh_market_data():
    """Refresh market data in background without blocking main thread"""
    global SYMBOLDICT, market_data_ready

    while True:
        try:
            start_time = time.time()
            print(f"Starting market data refresh at {datetime.now().strftime('%H:%M:%S')}")

            temp_data = {index: {} for index in indices.keys()}
            skipped_sensex = False

            for index_name in indices.keys():
                if not SYMBOLDICT[index_name]:
                    continue

                # Skip Sensex if we previously detected invalid data
                if index_name == "SENSEX" and skipped_sensex:
                    print("Skipping SENSEX refresh due to previous invalid data")
                    continue

                tokens = list(SYMBOLDICT[index_name].keys())
                batch_size = 30  # Reduced batch size for faster response

                with ThreadPoolExecutor(max_workers=15) as executor:
                    futures = []
                    for i in range(0, len(tokens), batch_size):
                        batch = tokens[i:i + batch_size]
                        for token in batch:
                            exch, tok = token.split('|')
                            futures.append(executor.submit(api.get_quotes, exchange=exch, token=tok))

                    for future in futures:
                        try:
                            quote = future.result(timeout=3)
                            if quote:
                                # Special handling for Sensex - check for valid data
                                if index_name == "SENSEX":
                                    if all(v == 0 or v == '' or v is None for k, v in quote.items()
                                           if k in ['oi', 'v', 'bq1', 'bp1', 'sq1', 'sp1']):
                                        print("Invalid/zero data received for SENSEX, skipping...")
                                        skipped_sensex = True
                                        break

                                key = f"{quote['exch']}|{quote['token']}"
                                temp_data[index_name][key] = {
                                    'oi': quote.get('oi'),
                                    'v': quote.get('v'),
                                    # 'bq1': quote.get('bq1'),
                                    # 'bp1': quote.get('bp1'),
                                    # 'sq1': quote.get('sq1'),
                                    # 'sp1': quote.get('sp1'),
                                    'lp' : quote.get('lp'),
                                    'tbq': quote.get('tbq'),
                                    'tsq': quote.get('tsq'),
                                    # 'ap': quote.get('ap'),
                                    'pc': quote.get('pc'),
                                    'ls': quote.get('ls')
                                }

                        except Exception as e:
                            print(f"Error getting quote: {e}")
                            if index_name == "SENSEX":
                                skipped_sensex = True

                    # If we broke out of the loop due to invalid Sensex data, skip remaining futures
                    if index_name == "SENSEX" and skipped_sensex:
                        break

            # Update SYMBOLDICT with market data lock
            with market_data_lock:
                for index_name in indices.keys():
                    # Skip updating Sensex if we had invalid data
                    if index_name == "SENSEX" and skipped_sensex:
                        print("Skipping SENSEX data update due to invalid data")
                        continue

                    for key, values in temp_data[index_name].items():
                        if key in SYMBOLDICT[index_name]:
                            SYMBOLDICT[index_name][key].update(values)

                market_data_ready = True

            duration = time.time() - start_time
            print(f"Market data refresh completed in {duration:.2f} seconds")

            # Adjust sleep time based on how long the refresh took
            sleep_time = max(1, control_params["market_data_refresh_interval"] - duration)
            time.sleep(sleep_time)

        except Exception as e:
            print(f"Error in market data refresh: {e}")
            time.sleep(5)

# Read symbols for all indices
print("Reading symbols for all indices...")
symbols_by_index = {index: read_symbols('NFO_SYMBOLS.txt', index) for index in indices.keys()}

# Group symbols by index and expiry date
print("Grouping symbols by index and expiry date...")
expiry_groups_by_index = {index: defaultdict(list) for index in indices.keys()}
for index, symbols in symbols_by_index.items():
    for symbol in symbols:
        expiry_groups_by_index[index][symbol["exd"]].append(symbol)
print("Symbols grouped.")

# Sort expiry dates in ascending order for each index and slice to the requested expiry dates
print("Sorting and slicing expiry dates...")
sorted_expiry_dates_by_index = {
    index: sorted(expiry_groups.keys())[:control_params["expiry_dates_count"]]  # Get the specified number of expiry dates
    for index, expiry_groups in expiry_groups_by_index.items()
}
print("Sorted and sliced expiry dates by index:")
for index, expiry_dates in sorted_expiry_dates_by_index.items():
    print(f"{index}: {[date.strftime('%d-%b-%Y') for date in expiry_dates]}")

# Function to get last quote
def getLastQuote(scrip):
    if not scrip or "exch" not in scrip or "token" not in scrip:
        print("Invalid scrip data. Skipping...")
        return None
    scripdata = api.get_quotes(exchange=scrip['exch'], token=scrip['token'])
    return scripdata

# Get option chain quotes for a specific expiry date
def getLastQuoteOptionChain(exchange, tradingsymbol, strikeprice, count=control_params["get_quotes_count"]):
    chain = api.get_option_chain(exchange=exchange, tradingsymbol=tradingsymbol, strikeprice=strikeprice, count=count)
    if not chain or "values" not in chain:
        print("No valid option chain data found.")
        return []
    with ThreadPoolExecutor(max_workers=10) as exe:  # Reduced max_workers
        result = list(exe.map(getLastQuote, chain['values']))
    return result

feed_opened = False
SYMBOLDICT = {index: {} for index in indices.keys()}
live_data = {index: {} for index in indices.keys()}

# Live market data update
def event_handler_feed_update(tick_data):
    # Instead of processing directly, add to queue
    tick_queue.put(tick_data)

# Process tick data from the queue
def process_tick_data():
    while not tick_queue.empty():
        try:
            tick_data = tick_queue.get(block=False)
            #print("Tick data:",tick_data)

            # Skip heartbeat messages
            if tick_data.get('t') == '' and tick_data.get('e') == 'wo':
                continue

            # Extract all relevant fields from tick_data
            fields = [
                "ts", "ls", "lp", "pc", "c", "o", "h", "l", "v",
                "ltq", "ltp", "ap", "oi", "poi", "toi",
                "bq1", "bp1", "bq2", "bp2", "bq3", "bp3",
                "sq1", "sp1", "sq2", "sp2", "sq3", "sp3",
                "tbq", "tsq", "bq", "bp", "sq", "sp"
            ]
            message = {field: tick_data[field] for field in fields if field in tick_data}
            #print(f"Tick data: {message}")

            # Construct the key
            key = f"{tick_data['e']}|{tick_data['tk']}"

            # Safe field access with defaults
            key_fields = {
                'oi': tick_data.get('oi', 0),          # Default to 0 if missing
                'poi': tick_data.get('poi', 0),        # Previous OI
                'tbq': tick_data.get('tbq', 0),        # Total Buy Quantity
                'tsq': tick_data.get('tsq', 0),       # Total Sell Quantity
                'token': tick_data.get('tk', ''),
                'exchange': tick_data.get('e', ''),
                'timestamp': time.time()
            }

            print(f"Tick data received - OI: {key_fields['oi']}, POI: {key_fields['poi']}, "
                  f"TBQ: {key_fields['tbq']}, TSQ: {key_fields['tsq']} "
                  f"for token {key_fields['token']} at {key_fields['timestamp']}")

            # Determine the index
            index = None
            tsym = tick_data.get("ts", "")
            if tsym:  # If tsym is present, extract index from it
                for idx in indices.keys():
                    if tsym.startswith(idx):  # Check if the trading symbol starts with the index name
                        index = idx
                        break
            else:  # If tsym is missing, use token-to-index mapping
                token = tick_data.get("tk", "")
                index = token_to_index.get(token, None)

            # Update SYMBOLDICT if the index is found
            if index:
                if key in SYMBOLDICT[index]:
                    SYMBOLDICT[index][key].update(message)
                else:
                    SYMBOLDICT[index][key] = message

            tick_queue.task_done()
        except queue.Empty:
            break
        except Exception as e:
            print(f"Error processing tick data: {e}")
            continue

def event_handler_order_update(order_update):
    print(f"Order feed update: {order_update}")

def open_callback():
    global feed_opened
    feed_opened = True
    print("WebSocket connected.")

# Start WebSocket
api.start_websocket(order_update_callback=event_handler_order_update,
                    subscribe_callback=event_handler_feed_update,
                    socket_open_callback=open_callback)

# Ensure WebSocket connection
print("Connecting to WebSocket...")
connection_timeout = 30  # timeout in seconds
start_time = time.time()
while not feed_opened:
    if time.time() - start_time > connection_timeout:
        print("WebSocket connection timed out. Exiting...")
        exit(1)
    print("Trying to connect WebSocket...")
    time.sleep(1)
print("WebSocket connected.")

# Import IV calculation function
from iv_cal import calculate_iv_with_calcivgreeks

def calculate_iv(row):
    """
    Calculate implied volatility for call and put options.

    Args:
        row (pd.Series or dict): A row of option data.

    Returns:
        Tuple[float, float]: Implied volatility for call and put options.
    """
    try:
        # Use the new IV calculation method
        iv_ce, iv_pe = calculate_iv_with_calcivgreeks(row)
        return iv_ce, iv_pe
    except Exception as e:
        print(f"Error calculating IV: {e}")
        return 0, 0  # Return zeros on error

# Global token-to-index mapping
token_to_index = {}

def subscribe_to_tokens(index_name, expiry_groups, strike_price):
    dtatlist = []
    for expiry_date in sorted_expiry_dates_by_index[index_name]:
        print(f"Processing expiry date: {expiry_date.strftime('%d-%b-%Y')} for {index_name}")
        symbols = expiry_groups[expiry_date]

        # Process all symbols for each expiry date, not just the first one
        # Change this to process more symbols if needed, but be careful of rate limits
        symbols_to_process = symbols  # Process all symbols for each expiry

        for symbol in symbols_to_process[:1]:
            if index_name == "SENSEX":
                update_sensex_spot_price()
                strike_price = current_sensex_price if current_sensex_price > 0 else control_params["sensex_spot_price"]
                print(f"Symbol: {symbol['tsym']}, Using Spot Price: {strike_price}")

            # Add a delay before calling getLastQuoteOptionChain
            time.sleep(control_params["sleep_time"])

            quotes = getLastQuoteOptionChain(exchange=symbol["exch"], tradingsymbol=symbol["tsym"], strikeprice=strike_price, count=control_params["get_quotes_count"])
            for scrip in quotes:
                if not scrip:
                    continue
                try:
                    key = f"{scrip['exch']}|{scrip['token']}"
                    dtatlist.append(key)

                    # For SENSEX, always use the current spot price
                    spot_price = current_sensex_price if index_name == "SENSEX" else strike_price
                    if index_name == "SENSEX":
                        print("Sensex spot price:", spot_price)

                    # Populate live_data with initial data, making sure to properly track expiry date
                    live_data[index_name][key] = {
                        "Type": scrip["optt"],
                        "strike": float(scrip['strprc']),
                        "o": scrip.get("o", 0),
                        "h": scrip.get("h", 0),
                        "l": scrip.get("l", 0),
                        "c": scrip.get("c", 0),
                        "v": scrip.get("v", 0),
                        "spot": spot_price,
                        "tsym": scrip["tsym"],
                        "exd": symbol["exd"].strftime('%d-%b-%Y'),  # Make sure expiry date is properly stored
                        "poi": scrip.get("poi", 0),
                        "bq1": scrip.get("bq1", 0),
                        "bp1": scrip.get("bp1", 0),
                        "bq2": scrip.get("bq2", 0),
                        "bp2": scrip.get("bp2", 0),
                        "sq1": scrip.get("sq1", 0),
                        "sp1": scrip.get("sp1", 0),
                        "sq2": scrip.get("sq2", 0),
                        "sp2": scrip.get("sp2", 0),
                        "lp": scrip.get("lp", 0),
                        "tbq": scrip.get("tbq", 0),
                        "tsq": scrip.get("tsq", 0),
                        "pc": scrip.get("pc", 0),
                        "ls": scrip.get("ls", 0),
                        "ap": scrip.get("ap", 0)
                    }

                    # Store token-to-index mapping
                    token_to_index[scrip['token']] = index_name
                except KeyError as e:
                    print(f"Missing key in scrip data: {e}. Scrip: {scrip}")

    # Update SYMBOLDICT with live_data for this index
    SYMBOLDICT[index_name].update(live_data[index_name])
    return dtatlist

def subscribe_with_retry(index_name, details, max_retries=3, retry_delay=5):
    global current_sensex_price

    retry_count = 0
    while retry_count < max_retries:
        try:
            token = details["token"]
            if index_name == "SENSEX":
                # First try to get the live scraped price
                if current_sensex_price > 0:
                    strike_price = current_sensex_price
                    print(f"Using live Sensex spot price: {strike_price}")
                else:
                    # Fall back to Excel value if scraping failed
                    strike_price = control_params["sensex_spot_price"]
                    print(f"Using Sensex spot price from Excel: {strike_price}")
            else:
                market_data = api.get_quotes(exchange="NSE", token=token)
                if market_data is None or market_data.get("stat") != "Ok":
                    print(f"Failed to fetch market data for {index_name}. Retry {retry_count + 1}/{max_retries}...")
                    retry_count += 1
                    time.sleep(retry_delay)
                    continue
                strike_price = float(market_data.get("lp", 0))

            dtatlist = subscribe_to_tokens(index_name, expiry_groups_by_index[index_name], strike_price)
            if dtatlist:
                api.subscribe(dtatlist)
                print(f"Subscribed {len(dtatlist)} tokens for {index_name}")
                return True  # Success
            else:
                print(f"No tokens to subscribe for {index_name}")
                return False
        except Exception as e:
            print(f"Error subscribing to tokens for {index_name}: {e}. Retry {retry_count + 1}/{max_retries}...")
            retry_count += 1
            time.sleep(retry_delay)

    print(f"Failed to subscribe to tokens for {index_name} after {max_retries} retries. Skipping...")
    return False  # Failure
# Modified process_and_write_index function
def process_and_write_index(index_name, details):
    global market_data_ready

    try:
        sheet = details["sheet"]

        # Skip if no data for this index
        if not SYMBOLDICT[index_name]:
            print(f"No data available for {index_name}. Skipping...")
            return

        # Create a local copy of the data with market data lock
        with market_data_lock:
            df = pd.DataFrame.from_dict(SYMBOLDICT[index_name], orient="index")

        # Skip if DataFrame is empty
        if df.empty:
            print(f"Empty DataFrame for {index_name}. Skipping...")
            return

        print(f"Processing data for index: {index_name}")

        # Ensure necessary columns exist - add default values if missing
        required_cols = ["Type", "strike", "exd", "oi", "poi", "v", "ls", "pc", "ap"]
        for col in required_cols:
            if col not in df.columns:
                df[col] = 0  # Default to 0 if column is missing
                print(f"Added missing column {col} with default 0 for {index_name}")

        # Create a dictionary to store data by expiry date
        data_by_expiry = {}

        # Get unique expiry dates in the data
        unique_expiry_dates = df['exd'].unique()
        print(f"Found {len(unique_expiry_dates)} unique expiry dates for {index_name}: {unique_expiry_dates}")

        # Process each expiry date separately
        for expiry_date in unique_expiry_dates:
            # Skip empty expiry dates
            if not expiry_date:
                continue

            print(f"Processing {index_name} data for expiry: {expiry_date}")

            # Filter data for this specific expiry date
            expiry_df = df[df.exd == expiry_date].copy()

            # Skip if no data for this expiry date
            if expiry_df.empty:
                print(f"No data for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            # Process Call and Put options for this expiry
            cdf = expiry_df[expiry_df.Type == "CE"].copy()
            pdf = expiry_df[expiry_df.Type == "PE"].copy()

            # If no calls or puts, skip processing this expiry
            if cdf.empty and pdf.empty:
                print(f"No CE or PE data for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            # Rename columns for calls - only if they exist
            ce_rename_map = {
                "oi": "oi_ce",
                "poi": "poi_ce",
                "v": "v_ce",
                "ls": "ls_ce",
                "pc": "pc_ce",
                "ap": "ap_ce",
                "bq1": "bq1_ce",
                "bp1": "bp1_ce",
                "bq2": "bq2_ce",
                "bp2": "bp2_ce",
                "sq1": "sq1_ce",
                "sp1": "sp1_ce",
                "sq2": "sq2_ce",
                "sp2": "sp2_ce",
                "lp": "ltp_ce",
                "tbq": "tbq_ce",
                "tsq": "tsq_ce",
                "o": "o_ce",
                "h": "h_ce",
                "l": "l_ce",
                "c": "c_ce"
            }

            pe_rename_map = {
                "oi": "oi_pe",
                "poi": "poi_pe",
                "v": "v_pe",
                "ls": "ls_pe",
                "pc": "pc_pe",
                "ap": "ap_pe",
                "bq1": "bq1_pe",
                "bp1": "bp1_pe",
                "bq2": "bq2_pe",
                "bp2": "bp2_pe",
                "sq1": "sq1_pe",
                "sp1": "sp1_pe",
                "sq2": "sq2_pe",
                "sp2": "sp2_pe",
                "lp": "ltp_pe",
                "tbq": "tbq_pe",
                "tsq": "tsq_pe",
                "o": "o_pe",
                "h": "h_pe",
                "l": "l_pe",
                "c": "c_pe"
            }

            # Only rename columns that actually exist in the DataFrame
            if not cdf.empty:
                existing_ce_cols = [col for col in ce_rename_map.keys() if col in cdf.columns]
                cdf.rename(columns={k: ce_rename_map[k] for k in existing_ce_cols}, inplace=True)

            if not pdf.empty:
                existing_pe_cols = [col for col in pe_rename_map.keys() if col in pdf.columns]
                pdf.rename(columns={k: pe_rename_map[k] for k in existing_pe_cols}, inplace=True)

            # Collect all strike prices for this expiry
            all_strikes = set()
            if not cdf.empty:
                all_strikes.update(cdf["strike"].dropna().tolist())
            if not pdf.empty:
                all_strikes.update(pdf["strike"].dropna().tolist())


            if not all_strikes:
                print(f"No valid strike prices for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            if not cdf.empty:
                cdf["oi_ce"] = pd.to_numeric(cdf["oi_ce"], errors="coerce")
                cdf["poi_ce"] = pd.to_numeric(cdf["poi_ce"], errors="coerce")
                cdf["coi_ce"] = cdf["oi_ce"] - cdf["poi_ce"]

            if not pdf.empty:
                pdf["oi_pe"] = pd.to_numeric(pdf["oi_pe"], errors="coerce")
                pdf["poi_pe"] = pd.to_numeric(pdf["poi_pe"], errors="coerce")
                pdf["coi_pe"] = pdf["oi_pe"] - pdf["poi_pe"]

            # Create DataFrame with unique strike prices for this expiry
            strikes = pd.DataFrame(sorted(all_strikes))
            strikes.columns = ["strike"]

            # Add expiry date column to keep track
            strikes['expiry_date'] = expiry_date

            # Merge strikes with call and put dataframes
            # Use outer join to keep all data
            if not cdf.empty and not pdf.empty:
                exp_chain = strikes.merge(cdf, on="strike", how="left").merge(pdf, on="strike", how="left", suffixes=('_ce', '_pe'))
            elif not cdf.empty:
                exp_chain = strikes.merge(cdf, on="strike", how="left")
            elif not pdf.empty:
                exp_chain = strikes.merge(pdf, on="strike", how="left")
            else:
                print(f"No valid data to merge for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            # Fill NaN values
            exp_chain.fillna(0, inplace=True)
            exp_chain = exp_chain.infer_objects(copy=False)

            # Ensure strike is numeric and sort
            exp_chain["strike"] = pd.to_numeric(exp_chain["strike"], errors="coerce")
            exp_chain = exp_chain.dropna(subset=["strike"])
            exp_chain.sort_values(by=["strike"], inplace=True, ascending=True)
            exp_chain.reset_index(drop=True, inplace=True)

            # Calculate implied volatility for this expiry
            iv_data = []
            for _, row in exp_chain.iterrows():
                try:
                    iv_ce, iv_pe = calculate_iv(row)
                    iv_data.append((iv_ce, iv_pe))
                except Exception as e:
                    print(f"Error calculating IV for row with strike {row.get('strike')}: {e}")
                    iv_data.append((0, 0))

            # Add IV columns
            exp_chain["iv_ce"] = [data[0] for data in iv_data]
            exp_chain["iv_pe"] = [data[1] for data in iv_data]

            # Store processed data for this expiry
            data_by_expiry[expiry_date] = exp_chain

        # If no valid data was processed for any expiry, exit
        if not data_by_expiry:
            print(f"No valid data processed for any expiry for {index_name}. Skipping...")
            return

        # Concatenate all expiry data
        OChain = pd.concat(data_by_expiry.values(), ignore_index=True)

        # Reindex columns to a consistent order
        preferred_columns = [
            "expiry_date",
            "oi_ce", "coi_ce", "v_ce", "iv_ce", "pc_ce", "ls_ce", "ap_ce",
            "bq1_ce", "bp1_ce", "bq2_ce", "bp2_ce",
            "sq1_ce", "sp1_ce", "sq2_ce", "sp2_ce",
            "ltp_ce", "tbq_ce", "tsq_ce", "o_ce", "h_ce", "l_ce", "c_ce",
            "strike", "exd_ce", "spot_ce",
            "oi_pe", "coi_pe", "v_pe", "iv_pe", "pc_pe", "ls_pe", "ap_pe",
            "bq1_pe", "bp1_pe", "bq2_pe", "bp2_pe",
            "sq1_pe", "sp1_pe", "sq2_pe", "sp2_pe",
            "ltp_pe", "tbq_pe", "tsq_pe", "o_pe", "h_pe", "l_pe", "c_pe"
        ]

        # Filter to only include columns that actually exist
        final_columns = [col for col in preferred_columns if col in OChain.columns]

        # Make sure 'strike' and 'expiry_date' are included
        for essential_col in ['strike', 'expiry_date']:
            if essential_col not in final_columns and essential_col in OChain.columns:
                final_columns.insert(0, essential_col)

        # Reindex columns
        OChain = OChain.reindex(columns=final_columns)

        # Sort by expiry date and then strike
        if 'expiry_date' in OChain.columns:
            OChain.sort_values(by=['expiry_date', 'strike'], inplace=True)
        else:
            OChain.sort_values(by=['strike'], inplace=True)

        OChain.reset_index(drop=True, inplace=True)

        # Get the sheet object from the workbook
        sheet_object = wb.sheets[sheet]  # Convert sheet name to sheet object

        # Add data to Excel queue
        excel_queue.put((sheet_object, OChain))  # Pass the sheet object, not the sheet name

    except Exception as e:
        print(f"Error processing {index_name}: {e}")
        traceback.print_exc()
def update_spot_prices():
    """Fetch latest spot prices for NIFTY and BANKNIFTY and update SYMBOLDICT."""
    global SYMBOLDICT

    for index_name in ["FINNIFTY", "MIDCPNIFTY"]:
        try:
            # Get live spot from NSE
            spot_data = api.get_quotes(exchange="NSE", token=indices[index_name]["token"])
            if spot_data and "lp" in spot_data:
                new_spot = float(spot_data["lp"])

                # Update spot price for all tokens in SYMBOLDICT
                for token_key in SYMBOLDICT[index_name]:
                    SYMBOLDICT[index_name][token_key]["spot"] = new_spot

                print(f"Updated {index_name} spot: {new_spot}")
        except Exception as e:
            print(f"Failed to update {index_name} spot: {e}")

# Function to write data to Excel from the queue
def write_to_excel():
    while True:
        try:
            sheet, data = excel_queue.get(block=False)
            # Log critical fields before writing
            print(f"\nWriting to Excel - Sample data for {sheet.name}:")
            print(data[['strike', 'oi_ce', 'coi_ce', 'tbq_ce', 'tsq_ce']].head())
            if not safe_excel_write(wb, sheet.name, data):  # Use sheet.name to get the sheet name
                print(f"Failed to write data for {sheet.name} to Excel.")
            else:
                print(f"Updated data in Excel for {sheet.name}")
            excel_queue.task_done()
        except queue.Empty:
            break
        except Exception as e:
            print(f"Error writing to Excel: {e}")

# Subscribe to tokens for all indices with retry mechanism
print("Subscribing to tokens for all indices...")
for index_name, details in indices.items():
    subscribe_with_retry(index_name, details)

# Track the last time Excel was updated
last_excel_update = time.time()

#start market data refresh thread
market_data_thread = threading.Thread(target=refresh_market_data, daemon=True)
market_data_thread.start()

try:
    while True:
        current_time = time.time()

        # Process incoming tick data
        process_tick_data()

        # Update spot prices every N seconds (e.g., every 10 sec)
        update_spot_prices()
        last_spot_update = current_time

        # Check if it's time for Excel update
        if current_time - last_excel_update >= control_params["update_interval"]:
            try:
                print(f"Starting Excel update at {datetime.now().strftime('%H:%M:%S')}")

                # Process and write all indices
                for index_name, details in indices.items():
                    process_and_write_index(index_name, details)

                # Write to Excel
                write_to_excel()

                last_excel_update = current_time
                print(f"Excel update completed at {datetime.now().strftime('%H:%M:%S')}")

            except Exception as e:
                print(f"Error during Excel update: {e}")

        time.sleep(0.1)  # Prevent CPU overload

except KeyboardInterrupt:
    print("Program interrupted by user. Exiting...")
except Exception as e:
    print(f"Unhandled exception: {e}")
finally:
    # Clean up resources
    print("Closing WebSocket and Excel...")
    api.close_websocket()
    wb.save()
    print("Program ended.")import time
import traceback
import pyotp
import xlwings as xw
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
from NorenRestApiPy.NorenApi import NorenApi
from collections import defaultdict
from datetime import datetime
import requests
import zipfile
import os
import queue
import threading
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import pandas as pd
pd.set_option('future.no_silent_downcasting', True)

# Function to download and extract files
def download_and_extract(url):
    """Download a zip file from the given URL and extract it to the specified path."""
    try:
        response = requests.get(url)
        if response.status_code == 200:
            zip_path = os.path.join(os.path.basename(url))
            with open(zip_path, "wb") as file:
                file.write(response.content)

            # Extract the zip file
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall()
            print("Extracted symbols")
        else:
            print(f"Failed to download {url}. Status code: {response.status_code}")
    except Exception as e:
        print(f"Error downloading/extracting {url}: {e}")

# Download and extract NFO and BFO symbols
download_and_extract("https://api.shoonya.com/NFO_symbols.txt.zip")
download_and_extract("https://api.shoonya.com/BFO_symbols.txt.zip")

class ShoonyaApiPy(NorenApi):
    def __init__(self):
        NorenApi.__init__(self, host='https://api.shoonya.com/NorenWClientTP/', websocket='wss://api.shoonya.com/NorenWSTP/')

api = ShoonyaApiPy()

# Create a message queue for tick data
tick_queue = queue.Queue()

# Create a queue for Excel updates
excel_queue = queue.Queue()

# Ensure Excel file exists (macro-enabled format)
excel_file = "option_futures.xlsm"
if not os.path.exists(excel_file):
    print(f"Excel file '{excel_file}' does not exist. Creating a new macro-enabled workbook...")
    wb = xw.Book()
    wb.save(excel_file)
    print(f"Created new macro-enabled workbook: {excel_file}")
else:
    print(f"Excel file '{excel_file}' already exists.")
    wb = xw.Book(excel_file)

# Define indices and their tokens
indices = {
       "FINNIFTY": {"token": "26037", "sheet": "finnifty"},
    "MIDCPNIFTY": {"token": "26074", "sheet": "midcpnifty"},
    "SENSEX": {"token": "BSXOPT", "sheet": "sensex"}
}

# Function to ensure sheets exist
def ensure_sheets_exist(wb, indices):
    # Add a new sheet named 'Control' if it doesn't exist
    if "Control" not in [sheet.name for sheet in wb.sheets]:
        print("Sheet 'Control' does not exist. Creating...")
        wb.sheets.add("Control")

    # Ensure sheets for indices exist
    for index, details in indices.items():
        sheet_name = details["sheet"]
        if sheet_name not in [sheet.name for sheet in wb.sheets]:
            print(f"Sheet '{sheet_name}' does not exist. Creating...")
            wb.sheets.add(sheet_name)
    return {index: wb.sheets[details["sheet"]] for index, details in indices.items()}

# Ensure sheets exist
print("Ensuring sheets exist...")
sheets = ensure_sheets_exist(wb, indices)
print("Sheets ensured.")

def read_control_parameters(wb):
    control_sheet = wb.sheets["Control"]
    sensex_spot = control_sheet.range("B8").value

    # Handle case where fetch failed is shown in Excel
    if isinstance(sensex_spot, str) and "Fetch Failed" in sensex_spot:
        sensex_spot = 0  # Will use the last known good value

    control_params = {
        "expiry_dates_count": int(control_sheet.range("B2").value or 1),
        "sleep_time": int(control_sheet.range("B3").value or 1),
        "get_quotes_count": int(control_sheet.range("B4").value or 10),
        "userid": str(control_sheet.range("B5").value or ""),
        "password": str(control_sheet.range("B6").value or ""),
        "totp_key": str(control_sheet.range("B7").value or ""),
        "sensex_spot_price": float(sensex_spot or 0),  # Handle None or string values
        "update_interval": 10,
        "sensex_update_interval": 240,
        "market_data_refresh_interval": 3  # Added new parameter for market data refresh
    }
    return control_params

# Initialize the spot update trackers
last_sensex_update = 0
current_sensex_price = 0
last_market_data_refresh = 0

# Read control parameters
print("Reading control parameters...")
control_params = read_control_parameters(wb)
print("Control parameters:", control_params)

# Login using credentials from the 'Control' sheet
def shoonya_login():
    global api, feed_opened
    try:
        # Generate TOTP
        totp = pyotp.TOTP(control_params["totp_key"]).now()

        # Login to Shoonya API
        ret = api.login(
            userid=control_params["userid"],
            password=control_params["password"],
            twoFA=totp,
            vendor_code="FN100758_U",
            api_secret="e2fd2b1eb4da1c54ed4a84e04dc592ea",
            imei="abc1234"
        )
        if ret.get("stat") != "Ok":
            print(f"Login failed: {ret.get('emsg', 'Unknown error')}")
            exit(1)
        print("Login successful.")
    except Exception as e:
        print(f"Error during login: {e}")
        exit(1)

# Perform login
shoonya_login()

# Read symbols from text file and group by index and expiry date
def read_symbols(file_path, index):
    symbols = []
    with open(file_path, 'r') as file:
        next(file)  # Skip header
        for line in file:
            parts = line.strip().split(',')
            if parts[3] == index:  # Filter by index
                expiry_date = datetime.strptime(parts[5], "%d-%b-%Y")
                symbols.append({
                    "exch": parts[0],
                    "token": parts[1],
                    "tsym": parts[4],
                    "exd": expiry_date,
                    "instname": parts[6],
                    "optt": parts[7],
                    "strprc": parts[8]
                })
    return symbols

# Function to safely write to Excel
def safe_excel_write(wb, sheet_name, data, start_cell='B3'):
    try:
        sheet = wb.sheets[sheet_name]
        sheet.range(start_cell).options(index=False, header=True).value = data
        return True
    except Exception as e:
        print(f"Error writing to {sheet_name}: {e}")
        return False

def update_sensex_spot_in_excel(wb, spot_price):
    try:
        control_sheet = wb.sheets["Control"]
        # If spot_price is a number, format it properly
        if isinstance(spot_price, (int, float)):
            control_sheet.range("B8").value = float(spot_price)
            print(f"Updated Sensex spot price in Excel: {spot_price}")
        else:
            # Handle string messages like "Fetch Failed"
            control_sheet.range("B8").value = spot_price
            print(f"Updated Sensex spot status in Excel: {spot_price}")
        return True
    except Exception as e:
        print(f"Error updating Sensex spot price in Excel: {e}")
        return False

def update_sensex_spot_price():
    global current_sensex_price, last_sensex_update

    # Check if it's time to update the Sensex spot price
    current_time = time.time()
    if current_time - last_sensex_update >= control_params["sensex_update_interval"]:
        scraped_price = None
        scrape_successful = False

        # First try to scrape the live price
        print("Attempting to scrape live Sensex spot price...")
        try:
            # Initialize Chrome options for headless browsing
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--window-size=1920,1080")

            driver = webdriver.Chrome(options=chrome_options)

            try:
                print("Opening BSE website...")
                driver.get('https://www.bseindia.com/')

                # Wait until the spot price is loaded
                print("Waiting for spot price element...")
                wait = WebDriverWait(driver, 10)
                spot_price_element = wait.until(EC.presence_of_element_located((By.ID, 'tdsp')))
                spot_price_text = spot_price_element.text
                print(f"Found spot price text: {spot_price_text}")

                # Clean the price text (remove commas) and convert to float
                scraped_price = float(spot_price_text.replace(',', ''))
                scrape_successful = True
                print(f"Successfully scraped Sensex spot price: {scraped_price}")

            except Exception as e:
                print(f"Failed to scrape Sensex spot price: {str(e)}")
                traceback.print_exc()

            finally:
                driver.quit()
                print("Web driver closed")

        except Exception as e:
            print(f"Error in Selenium setup: {str(e)}")
            traceback.print_exc()

        # Determine which price to use
        if scrape_successful and scraped_price is not None and scraped_price > 0:
            # Use the scraped price if available and valid
            current_sensex_price = scraped_price
            # Update Excel with the new price
            update_sensex_spot_in_excel(wb, scraped_price)
            print(f"Using live scraped Sensex price: {scraped_price}")
        else:
            # Fall back to Excel value if scraping failed
            excel_price = control_params["sensex_spot_price"]
            if excel_price > 0:
                current_sensex_price = excel_price
                print(f"Scraping failed! Using Sensex spot price from Excel: {excel_price}")
                # Update Excel to show fetch failed but keep the existing value
                update_sensex_spot_in_excel(wb, f"Fetch Failed (Using: {excel_price})")
            else:
                current_sensex_price = 0
                print("Scraping failed and no valid Excel price available!")
                update_sensex_spot_in_excel(wb, "Fetch Failed")

        last_sensex_update = current_time

    return current_sensex_price

# Add these global variables at the top
market_data_ready = False
market_data_lock = threading.Lock()

def refresh_market_data():
    """Refresh market data in background without blocking main thread"""
    global SYMBOLDICT, market_data_ready

    while True:
        try:
            start_time = time.time()
            print(f"Starting market data refresh at {datetime.now().strftime('%H:%M:%S')}")

            temp_data = {index: {} for index in indices.keys()}
            skipped_sensex = False

            for index_name in indices.keys():
                if not SYMBOLDICT[index_name]:
                    continue

                # Skip Sensex if we previously detected invalid data
                if index_name == "SENSEX" and skipped_sensex:
                    print("Skipping SENSEX refresh due to previous invalid data")
                    continue

                tokens = list(SYMBOLDICT[index_name].keys())
                batch_size = 30  # Reduced batch size for faster response

                with ThreadPoolExecutor(max_workers=15) as executor:
                    futures = []
                    for i in range(0, len(tokens), batch_size):
                        batch = tokens[i:i + batch_size]
                        for token in batch:
                            exch, tok = token.split('|')
                            futures.append(executor.submit(api.get_quotes, exchange=exch, token=tok))

                    for future in futures:
                        try:
                            quote = future.result(timeout=3)
                            if quote:
                                # Special handling for Sensex - check for valid data
                                if index_name == "SENSEX":
                                    if all(v == 0 or v == '' or v is None for k, v in quote.items()
                                           if k in ['oi', 'v', 'bq1', 'bp1', 'sq1', 'sp1']):
                                        print("Invalid/zero data received for SENSEX, skipping...")
                                        skipped_sensex = True
                                        break

                                key = f"{quote['exch']}|{quote['token']}"
                                temp_data[index_name][key] = {
                                    'oi': quote.get('oi'),
                                    'v': quote.get('v'),
                                    # 'bq1': quote.get('bq1'),
                                    # 'bp1': quote.get('bp1'),
                                    # 'sq1': quote.get('sq1'),
                                    # 'sp1': quote.get('sp1'),
                                    'lp' : quote.get('lp'),
                                    'tbq': quote.get('tbq'),
                                    'tsq': quote.get('tsq'),
                                    # 'ap': quote.get('ap'),
                                    'pc': quote.get('pc'),
                                    'ls': quote.get('ls')
                                }

                        except Exception as e:
                            print(f"Error getting quote: {e}")
                            if index_name == "SENSEX":
                                skipped_sensex = True

                    # If we broke out of the loop due to invalid Sensex data, skip remaining futures
                    if index_name == "SENSEX" and skipped_sensex:
                        break

            # Update SYMBOLDICT with market data lock
            with market_data_lock:
                for index_name in indices.keys():
                    # Skip updating Sensex if we had invalid data
                    if index_name == "SENSEX" and skipped_sensex:
                        print("Skipping SENSEX data update due to invalid data")
                        continue

                    for key, values in temp_data[index_name].items():
                        if key in SYMBOLDICT[index_name]:
                            SYMBOLDICT[index_name][key].update(values)

                market_data_ready = True

            duration = time.time() - start_time
            print(f"Market data refresh completed in {duration:.2f} seconds")

            # Adjust sleep time based on how long the refresh took
            sleep_time = max(1, control_params["market_data_refresh_interval"] - duration)
            time.sleep(sleep_time)

        except Exception as e:
            print(f"Error in market data refresh: {e}")
            time.sleep(5)

# Read symbols for all indices
print("Reading symbols for all indices...")
symbols_by_index = {index: read_symbols('NFO_SYMBOLS.txt', index) for index in indices.keys()}

# Group symbols by index and expiry date
print("Grouping symbols by index and expiry date...")
expiry_groups_by_index = {index: defaultdict(list) for index in indices.keys()}
for index, symbols in symbols_by_index.items():
    for symbol in symbols:
        expiry_groups_by_index[index][symbol["exd"]].append(symbol)
print("Symbols grouped.")

# Sort expiry dates in ascending order for each index and slice to the requested expiry dates
print("Sorting and slicing expiry dates...")
sorted_expiry_dates_by_index = {
    index: sorted(expiry_groups.keys())[:control_params["expiry_dates_count"]]  # Get the specified number of expiry dates
    for index, expiry_groups in expiry_groups_by_index.items()
}
print("Sorted and sliced expiry dates by index:")
for index, expiry_dates in sorted_expiry_dates_by_index.items():
    print(f"{index}: {[date.strftime('%d-%b-%Y') for date in expiry_dates]}")

# Function to get last quote
def getLastQuote(scrip):
    if not scrip or "exch" not in scrip or "token" not in scrip:
        print("Invalid scrip data. Skipping...")
        return None
    scripdata = api.get_quotes(exchange=scrip['exch'], token=scrip['token'])
    return scripdata

# Get option chain quotes for a specific expiry date
def getLastQuoteOptionChain(exchange, tradingsymbol, strikeprice, count=control_params["get_quotes_count"]):
    chain = api.get_option_chain(exchange=exchange, tradingsymbol=tradingsymbol, strikeprice=strikeprice, count=count)
    if not chain or "values" not in chain:
        print("No valid option chain data found.")
        return []
    with ThreadPoolExecutor(max_workers=10) as exe:  # Reduced max_workers
        result = list(exe.map(getLastQuote, chain['values']))
    return result

feed_opened = False
SYMBOLDICT = {index: {} for index in indices.keys()}
live_data = {index: {} for index in indices.keys()}

# Live market data update
def event_handler_feed_update(tick_data):
    # Instead of processing directly, add to queue
    tick_queue.put(tick_data)

# Process tick data from the queue
def process_tick_data():
    while not tick_queue.empty():
        try:
            tick_data = tick_queue.get(block=False)
            #print("Tick data:",tick_data)

            # Skip heartbeat messages
            if tick_data.get('t') == '' and tick_data.get('e') == 'wo':
                continue

            # Extract all relevant fields from tick_data
            fields = [
                "ts", "ls", "lp", "pc", "c", "o", "h", "l", "v",
                "ltq", "ltp", "ap", "oi", "poi", "toi",
                "bq1", "bp1", "bq2", "bp2", "bq3", "bp3",
                "sq1", "sp1", "sq2", "sp2", "sq3", "sp3",
                "tbq", "tsq", "bq", "bp", "sq", "sp"
            ]
            message = {field: tick_data[field] for field in fields if field in tick_data}
            #print(f"Tick data: {message}")

            # Construct the key
            key = f"{tick_data['e']}|{tick_data['tk']}"

            # Safe field access with defaults
            key_fields = {
                'oi': tick_data.get('oi', 0),          # Default to 0 if missing
                'poi': tick_data.get('poi', 0),        # Previous OI
                'tbq': tick_data.get('tbq', 0),        # Total Buy Quantity
                'tsq': tick_data.get('tsq', 0),       # Total Sell Quantity
                'token': tick_data.get('tk', ''),
                'exchange': tick_data.get('e', ''),
                'timestamp': time.time()
            }

            print(f"Tick data received - OI: {key_fields['oi']}, POI: {key_fields['poi']}, "
                  f"TBQ: {key_fields['tbq']}, TSQ: {key_fields['tsq']} "
                  f"for token {key_fields['token']} at {key_fields['timestamp']}")

            # Determine the index
            index = None
            tsym = tick_data.get("ts", "")
            if tsym:  # If tsym is present, extract index from it
                for idx in indices.keys():
                    if tsym.startswith(idx):  # Check if the trading symbol starts with the index name
                        index = idx
                        break
            else:  # If tsym is missing, use token-to-index mapping
                token = tick_data.get("tk", "")
                index = token_to_index.get(token, None)

            # Update SYMBOLDICT if the index is found
            if index:
                if key in SYMBOLDICT[index]:
                    SYMBOLDICT[index][key].update(message)
                else:
                    SYMBOLDICT[index][key] = message

            tick_queue.task_done()
        except queue.Empty:
            break
        except Exception as e:
            print(f"Error processing tick data: {e}")
            continue

def event_handler_order_update(order_update):
    print(f"Order feed update: {order_update}")

def open_callback():
    global feed_opened
    feed_opened = True
    print("WebSocket connected.")

# Start WebSocket
api.start_websocket(order_update_callback=event_handler_order_update,
                    subscribe_callback=event_handler_feed_update,
                    socket_open_callback=open_callback)

# Ensure WebSocket connection
print("Connecting to WebSocket...")
connection_timeout = 30  # timeout in seconds
start_time = time.time()
while not feed_opened:
    if time.time() - start_time > connection_timeout:
        print("WebSocket connection timed out. Exiting...")
        exit(1)
    print("Trying to connect WebSocket...")
    time.sleep(1)
print("WebSocket connected.")

# Import IV calculation function
from iv_cal import calculate_iv_with_calcivgreeks

def calculate_iv(row):
    """
    Calculate implied volatility for call and put options.

    Args:
        row (pd.Series or dict): A row of option data.

    Returns:
        Tuple[float, float]: Implied volatility for call and put options.
    """
    try:
        # Use the new IV calculation method
        iv_ce, iv_pe = calculate_iv_with_calcivgreeks(row)
        return iv_ce, iv_pe
    except Exception as e:
        print(f"Error calculating IV: {e}")
        return 0, 0  # Return zeros on error

# Global token-to-index mapping
token_to_index = {}

def subscribe_to_tokens(index_name, expiry_groups, strike_price):
    dtatlist = []
    for expiry_date in sorted_expiry_dates_by_index[index_name]:
        print(f"Processing expiry date: {expiry_date.strftime('%d-%b-%Y')} for {index_name}")
        symbols = expiry_groups[expiry_date]

        # Process all symbols for each expiry date, not just the first one
        # Change this to process more symbols if needed, but be careful of rate limits
        symbols_to_process = symbols  # Process all symbols for each expiry

        for symbol in symbols_to_process[:1]:
            if index_name == "SENSEX":
                update_sensex_spot_price()
                strike_price = current_sensex_price if current_sensex_price > 0 else control_params["sensex_spot_price"]
                print(f"Symbol: {symbol['tsym']}, Using Spot Price: {strike_price}")

            # Add a delay before calling getLastQuoteOptionChain
            time.sleep(control_params["sleep_time"])

            quotes = getLastQuoteOptionChain(exchange=symbol["exch"], tradingsymbol=symbol["tsym"], strikeprice=strike_price, count=control_params["get_quotes_count"])
            for scrip in quotes:
                if not scrip:
                    continue
                try:
                    key = f"{scrip['exch']}|{scrip['token']}"
                    dtatlist.append(key)

                    # For SENSEX, always use the current spot price
                    spot_price = current_sensex_price if index_name == "SENSEX" else strike_price
                    if index_name == "SENSEX":
                        print("Sensex spot price:", spot_price)

                    # Populate live_data with initial data, making sure to properly track expiry date
                    live_data[index_name][key] = {
                        "Type": scrip["optt"],
                        "strike": float(scrip['strprc']),
                        "o": scrip.get("o", 0),
                        "h": scrip.get("h", 0),
                        "l": scrip.get("l", 0),
                        "c": scrip.get("c", 0),
                        "v": scrip.get("v", 0),
                        "spot": spot_price,
                        "tsym": scrip["tsym"],
                        "exd": symbol["exd"].strftime('%d-%b-%Y'),  # Make sure expiry date is properly stored
                        "poi": scrip.get("poi", 0),
                        "bq1": scrip.get("bq1", 0),
                        "bp1": scrip.get("bp1", 0),
                        "bq2": scrip.get("bq2", 0),
                        "bp2": scrip.get("bp2", 0),
                        "sq1": scrip.get("sq1", 0),
                        "sp1": scrip.get("sp1", 0),
                        "sq2": scrip.get("sq2", 0),
                        "sp2": scrip.get("sp2", 0),
                        "lp": scrip.get("lp", 0),
                        "tbq": scrip.get("tbq", 0),
                        "tsq": scrip.get("tsq", 0),
                        "pc": scrip.get("pc", 0),
                        "ls": scrip.get("ls", 0),
                        "ap": scrip.get("ap", 0)
                    }

                    # Store token-to-index mapping
                    token_to_index[scrip['token']] = index_name
                except KeyError as e:
                    print(f"Missing key in scrip data: {e}. Scrip: {scrip}")

    # Update SYMBOLDICT with live_data for this index
    SYMBOLDICT[index_name].update(live_data[index_name])
    return dtatlist

def subscribe_with_retry(index_name, details, max_retries=3, retry_delay=5):
    global current_sensex_price

    retry_count = 0
    while retry_count < max_retries:
        try:
            token = details["token"]
            if index_name == "SENSEX":
                # First try to get the live scraped price
                if current_sensex_price > 0:
                    strike_price = current_sensex_price
                    print(f"Using live Sensex spot price: {strike_price}")
                else:
                    # Fall back to Excel value if scraping failed
                    strike_price = control_params["sensex_spot_price"]
                    print(f"Using Sensex spot price from Excel: {strike_price}")
            else:
                market_data = api.get_quotes(exchange="NSE", token=token)
                if market_data is None or market_data.get("stat") != "Ok":
                    print(f"Failed to fetch market data for {index_name}. Retry {retry_count + 1}/{max_retries}...")
                    retry_count += 1
                    time.sleep(retry_delay)
                    continue
                strike_price = float(market_data.get("lp", 0))

            dtatlist = subscribe_to_tokens(index_name, expiry_groups_by_index[index_name], strike_price)
            if dtatlist:
                api.subscribe(dtatlist)
                print(f"Subscribed {len(dtatlist)} tokens for {index_name}")
                return True  # Success
            else:
                print(f"No tokens to subscribe for {index_name}")
                return False
        except Exception as e:
            print(f"Error subscribing to tokens for {index_name}: {e}. Retry {retry_count + 1}/{max_retries}...")
            retry_count += 1
            time.sleep(retry_delay)

    print(f"Failed to subscribe to tokens for {index_name} after {max_retries} retries. Skipping...")
    return False  # Failure
# Modified process_and_write_index function
def process_and_write_index(index_name, details):
    global market_data_ready

    try:
        sheet = details["sheet"]

        # Skip if no data for this index
        if not SYMBOLDICT[index_name]:
            print(f"No data available for {index_name}. Skipping...")
            return

        # Create a local copy of the data with market data lock
        with market_data_lock:
            df = pd.DataFrame.from_dict(SYMBOLDICT[index_name], orient="index")

        # Skip if DataFrame is empty
        if df.empty:
            print(f"Empty DataFrame for {index_name}. Skipping...")
            return

        print(f"Processing data for index: {index_name}")

        # Ensure necessary columns exist - add default values if missing
        required_cols = ["Type", "strike", "exd", "oi", "poi", "v", "ls", "pc", "ap"]
        for col in required_cols:
            if col not in df.columns:
                df[col] = 0  # Default to 0 if column is missing
                print(f"Added missing column {col} with default 0 for {index_name}")

        # Create a dictionary to store data by expiry date
        data_by_expiry = {}

        # Get unique expiry dates in the data
        unique_expiry_dates = df['exd'].unique()
        print(f"Found {len(unique_expiry_dates)} unique expiry dates for {index_name}: {unique_expiry_dates}")

        # Process each expiry date separately
        for expiry_date in unique_expiry_dates:
            # Skip empty expiry dates
            if not expiry_date:
                continue

            print(f"Processing {index_name} data for expiry: {expiry_date}")

            # Filter data for this specific expiry date
            expiry_df = df[df.exd == expiry_date].copy()

            # Skip if no data for this expiry date
            if expiry_df.empty:
                print(f"No data for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            # Process Call and Put options for this expiry
            cdf = expiry_df[expiry_df.Type == "CE"].copy()
            pdf = expiry_df[expiry_df.Type == "PE"].copy()

            # If no calls or puts, skip processing this expiry
            if cdf.empty and pdf.empty:
                print(f"No CE or PE data for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            # Rename columns for calls - only if they exist
            ce_rename_map = {
                "oi": "oi_ce",
                "poi": "poi_ce",
                "v": "v_ce",
                "ls": "ls_ce",
                "pc": "pc_ce",
                "ap": "ap_ce",
                "bq1": "bq1_ce",
                "bp1": "bp1_ce",
                "bq2": "bq2_ce",
                "bp2": "bp2_ce",
                "sq1": "sq1_ce",
                "sp1": "sp1_ce",
                "sq2": "sq2_ce",
                "sp2": "sp2_ce",
                "lp": "ltp_ce",
                "tbq": "tbq_ce",
                "tsq": "tsq_ce",
                "o": "o_ce",
                "h": "h_ce",
                "l": "l_ce",
                "c": "c_ce"
            }

            pe_rename_map = {
                "oi": "oi_pe",
                "poi": "poi_pe",
                "v": "v_pe",
                "ls": "ls_pe",
                "pc": "pc_pe",
                "ap": "ap_pe",
                "bq1": "bq1_pe",
                "bp1": "bp1_pe",
                "bq2": "bq2_pe",
                "bp2": "bp2_pe",
                "sq1": "sq1_pe",
                "sp1": "sp1_pe",
                "sq2": "sq2_pe",
                "sp2": "sp2_pe",
                "lp": "ltp_pe",
                "tbq": "tbq_pe",
                "tsq": "tsq_pe",
                "o": "o_pe",
                "h": "h_pe",
                "l": "l_pe",
                "c": "c_pe"
            }

            # Only rename columns that actually exist in the DataFrame
            if not cdf.empty:
                existing_ce_cols = [col for col in ce_rename_map.keys() if col in cdf.columns]
                cdf.rename(columns={k: ce_rename_map[k] for k in existing_ce_cols}, inplace=True)

            if not pdf.empty:
                existing_pe_cols = [col for col in pe_rename_map.keys() if col in pdf.columns]
                pdf.rename(columns={k: pe_rename_map[k] for k in existing_pe_cols}, inplace=True)

            # Collect all strike prices for this expiry
            all_strikes = set()
            if not cdf.empty:
                all_strikes.update(cdf["strike"].dropna().tolist())
            if not pdf.empty:
                all_strikes.update(pdf["strike"].dropna().tolist())


            if not all_strikes:
                print(f"No valid strike prices for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            if not cdf.empty:
                cdf["oi_ce"] = pd.to_numeric(cdf["oi_ce"], errors="coerce")
                cdf["poi_ce"] = pd.to_numeric(cdf["poi_ce"], errors="coerce")
                cdf["coi_ce"] = cdf["oi_ce"] - cdf["poi_ce"]

            if not pdf.empty:
                pdf["oi_pe"] = pd.to_numeric(pdf["oi_pe"], errors="coerce")
                pdf["poi_pe"] = pd.to_numeric(pdf["poi_pe"], errors="coerce")
                pdf["coi_pe"] = pdf["oi_pe"] - pdf["poi_pe"]

            # Create DataFrame with unique strike prices for this expiry
            strikes = pd.DataFrame(sorted(all_strikes))
            strikes.columns = ["strike"]

            # Add expiry date column to keep track
            strikes['expiry_date'] = expiry_date

            # Merge strikes with call and put dataframes
            # Use outer join to keep all data
            if not cdf.empty and not pdf.empty:
                exp_chain = strikes.merge(cdf, on="strike", how="left").merge(pdf, on="strike", how="left", suffixes=('_ce', '_pe'))
            elif not cdf.empty:
                exp_chain = strikes.merge(cdf, on="strike", how="left")
            elif not pdf.empty:
                exp_chain = strikes.merge(pdf, on="strike", how="left")
            else:
                print(f"No valid data to merge for {index_name} with expiry {expiry_date}. Skipping...")
                continue

            # Fill NaN values
            exp_chain.fillna(0, inplace=True)
            exp_chain = exp_chain.infer_objects(copy=False)

            # Ensure strike is numeric and sort
            exp_chain["strike"] = pd.to_numeric(exp_chain["strike"], errors="coerce")
            exp_chain = exp_chain.dropna(subset=["strike"])
            exp_chain.sort_values(by=["strike"], inplace=True, ascending=True)
            exp_chain.reset_index(drop=True, inplace=True)

            # Calculate implied volatility for this expiry
            iv_data = []
            for _, row in exp_chain.iterrows():
                try:
                    iv_ce, iv_pe = calculate_iv(row)
                    iv_data.append((iv_ce, iv_pe))
                except Exception as e:
                    print(f"Error calculating IV for row with strike {row.get('strike')}: {e}")
                    iv_data.append((0, 0))

            # Add IV columns
            exp_chain["iv_ce"] = [data[0] for data in iv_data]
            exp_chain["iv_pe"] = [data[1] for data in iv_data]

            # Store processed data for this expiry
            data_by_expiry[expiry_date] = exp_chain

        # If no valid data was processed for any expiry, exit
        if not data_by_expiry:
            print(f"No valid data processed for any expiry for {index_name}. Skipping...")
            return

        # Concatenate all expiry data
        OChain = pd.concat(data_by_expiry.values(), ignore_index=True)

        # Reindex columns to a consistent order
        preferred_columns = [
            "expiry_date",
            "oi_ce", "coi_ce", "v_ce", "iv_ce", "pc_ce", "ls_ce", "ap_ce",
            "bq1_ce", "bp1_ce", "bq2_ce", "bp2_ce",
            "sq1_ce", "sp1_ce", "sq2_ce", "sp2_ce",
            "ltp_ce", "tbq_ce", "tsq_ce", "o_ce", "h_ce", "l_ce", "c_ce",
            "strike", "exd_ce", "spot_ce",
            "oi_pe", "coi_pe", "v_pe", "iv_pe", "pc_pe", "ls_pe", "ap_pe",
            "bq1_pe", "bp1_pe", "bq2_pe", "bp2_pe",
            "sq1_pe", "sp1_pe", "sq2_pe", "sp2_pe",
            "ltp_pe", "tbq_pe", "tsq_pe", "o_pe", "h_pe", "l_pe", "c_pe"
        ]

        # Filter to only include columns that actually exist
        final_columns = [col for col in preferred_columns if col in OChain.columns]

        # Make sure 'strike' and 'expiry_date' are included
        for essential_col in ['strike', 'expiry_date']:
            if essential_col not in final_columns and essential_col in OChain.columns:
                final_columns.insert(0, essential_col)

        # Reindex columns
        OChain = OChain.reindex(columns=final_columns)

        # Sort by expiry date and then strike
        if 'expiry_date' in OChain.columns:
            OChain.sort_values(by=['expiry_date', 'strike'], inplace=True)
        else:
            OChain.sort_values(by=['strike'], inplace=True)

        OChain.reset_index(drop=True, inplace=True)

        # Get the sheet object from the workbook
        sheet_object = wb.sheets[sheet]  # Convert sheet name to sheet object

        # Add data to Excel queue
        excel_queue.put((sheet_object, OChain))  # Pass the sheet object, not the sheet name

    except Exception as e:
        print(f"Error processing {index_name}: {e}")
        traceback.print_exc()
def update_spot_prices():
    """Fetch latest spot prices for NIFTY and BANKNIFTY and update SYMBOLDICT."""
    global SYMBOLDICT

    for index_name in ["FINNIFTY", "MIDCPNIFTY"]:
        try:
            # Get live spot from NSE
            spot_data = api.get_quotes(exchange="NSE", token=indices[index_name]["token"])
            if spot_data and "lp" in spot_data:
                new_spot = float(spot_data["lp"])

                # Update spot price for all tokens in SYMBOLDICT
                for token_key in SYMBOLDICT[index_name]:
                    SYMBOLDICT[index_name][token_key]["spot"] = new_spot

                print(f"Updated {index_name} spot: {new_spot}")
        except Exception as e:
            print(f"Failed to update {index_name} spot: {e}")

# Function to write data to Excel from the queue
def write_to_excel():
    while True:
        try:
            sheet, data = excel_queue.get(block=False)
            # Log critical fields before writing
            print(f"\nWriting to Excel - Sample data for {sheet.name}:")
            print(data[['strike', 'oi_ce', 'coi_ce', 'tbq_ce', 'tsq_ce']].head())
            if not safe_excel_write(wb, sheet.name, data):  # Use sheet.name to get the sheet name
                print(f"Failed to write data for {sheet.name} to Excel.")
            else:
                print(f"Updated data in Excel for {sheet.name}")
            excel_queue.task_done()
        except queue.Empty:
            break
        except Exception as e:
            print(f"Error writing to Excel: {e}")

# Subscribe to tokens for all indices with retry mechanism
print("Subscribing to tokens for all indices...")
for index_name, details in indices.items():
    subscribe_with_retry(index_name, details)

# Track the last time Excel was updated
last_excel_update = time.time()

#start market data refresh thread
market_data_thread = threading.Thread(target=refresh_market_data, daemon=True)
market_data_thread.start()

try:
    while True:
        current_time = time.time()

        # Process incoming tick data
        process_tick_data()

        # Update spot prices every N seconds (e.g., every 10 sec)
        update_spot_prices()
        last_spot_update = current_time

        # Check if it's time for Excel update
        if current_time - last_excel_update >= control_params["update_interval"]:
            try:
                print(f"Starting Excel update at {datetime.now().strftime('%H:%M:%S')}")

                # Process and write all indices
                for index_name, details in indices.items():
                    process_and_write_index(index_name, details)

                # Write to Excel
                write_to_excel()

                last_excel_update = current_time
                print(f"Excel update completed at {datetime.now().strftime('%H:%M:%S')}")

            except Exception as e:
                print(f"Error during Excel update: {e}")

        time.sleep(0.1)  # Prevent CPU overload

except KeyboardInterrupt:
    print("Program interrupted by user. Exiting...")
except Exception as e:
    print(f"Unhandled exception: {e}")
finally:
    # Clean up resources
    print("Closing WebSocket and Excel...")
    api.close_websocket()
    wb.save()
    print("Program ended.")

